# Data_Science_Study_notebook

* For the Q&A_chatbot_generate_answer_with_GPT2.ipynb, we can see the loss funtion will stop to decrease when we train 21000 epoch, so we should stop to train more steps. Otherwise, the problem of overfitting will occur
